
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tutorial 2 – Training generative models &#8212; Quantum Tutorials</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "Alqor-UG/quantum-tutorials");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial 3 – An untrained generative model with qubits" href="generative_modeling_103.html" />
    <link rel="prev" title="Tutorial 1 – A baby example of generative modeling" href="generative_modeling_101.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Quantum Tutorials</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to our quantum tutorials
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  A beginners guide to QML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../beginner_qml/qml_intro.html">
   Course summary for a beginners guide to QML
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../beginner_qml/qml_001.html">
   QML 001 - A summary of classical supervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../beginner_qml/qml_101.html">
   QML 101 - Some basic concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../beginner_qml/qml_102.html">
   QML 102 - Deeper Classifiers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../beginner_qml/qml_103.html">
   QML 103 - Teaching the circle
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../beginner_qml/qml_104.html">
   QML 104 - More is different ? Working with multiple qubits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Generative modeling with quantum hardware
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="gmqh_intro.html">
   Summary of generative modeling with quantum hardware
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generative_modeling_101.html">
   Tutorial 1 – A baby example of generative modeling
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Tutorial 2 – Training generative models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generative_modeling_103.html">
   Tutorial 3 – An untrained generative model with qubits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generative_modeling_104.html">
   Tutorial 4 – A trained generative model with a few qubits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to quantum technologies
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../qtech_basics/qtech_summary.html">
   Course summary for introduction to quantum technologies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../qtech_basics/qtech_intro.html">
   Tutorial 1 - Basic concepts of quantum technologies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../qtech_basics/quantum_metrology.html">
   Tutorial 2 - Quantum metrology
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../qtech_basics/bell_inequalities.html">
   Tutorial on the nobel prize 2022 - How to win Bell’s game
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to quantum hardware
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../hardware_tutorial/qhw_intro.html">
   Course summary for introduction to quantum hardware
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hardware_tutorial/quantum_hardware_101.html">
   Tutorial 1 - The qubit or  the two level system
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hardware_tutorial/quantum_hardware_102.html">
   Tutorial 2 - Continuous variables or the quantum harmonic oscillator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hardware_tutorial/quantum_hardware_103.html">
   Tutorial 3 - A few words about quantum computing with trapped ions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hardware_tutorial/quantum_hardware_104.html">
   Tutorial 4 - A few words about quantum computing with superconducting qubits
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../hardware_tutorial/quantum_hardware_105.html">
   Tutorial 5 -Quantum simulation with cold atoms
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/generative_modeling/generative_modeling_102.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/Alqor-UG/quantum-tutorials"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/Alqor-UG/quantum-tutorials/issues/new?title=Issue%20on%20page%20%2Fgenerative_modeling/generative_modeling_102.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/Alqor-UG/quantum-tutorials/main?urlpath=tree/book/generative_modeling/generative_modeling_102.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-recap">
   A recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-by-training-the-generative-model">
   Learning by training the generative model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-and-outlook">
   Summary and outlook
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Tutorial 2 – Training generative models</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-recap">
   A recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-by-training-the-generative-model">
   Learning by training the generative model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-and-outlook">
   Summary and outlook
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="tutorial-2-training-generative-models">
<h1>Tutorial 2 – Training generative models<a class="headerlink" href="#tutorial-2-training-generative-models" title="Permalink to this headline">¶</a></h1>
<p>Welcome back to our tutorial on generative modeling! <a class="reference internal" href="generative_modeling_101.html#gmqh"><span class="std std-ref">Last time</span></a> we learned about some of the components in a generative modeling tasks, i.e., the <strong>the dataset</strong> and <strong>the model</strong>. However, we were still missing <strong>the learning</strong> part. This is what we will do today.</p>
<div class="section" id="a-recap">
<h2>A recap<a class="headerlink" href="#a-recap" title="Permalink to this headline">¶</a></h2>
<p>But first, lets recap:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># fmt: off
data_samples = np.array(
      [-3.61078109e-01, -1.29291442e-01, -9.82588388e-02, -2.58432923e-01,
       -4.97863965e-01, -4.78795061e-01, -5.10663163e-01,  4.36223299e-02,
       -1.01858250e-02, -5.57657880e-01, -5.61011437e-01, -2.63656495e-01,
       -5.00136079e-01, -6.30125377e-01, -1.12999295e-01, -3.22838879e-01,
       -7.75881873e-01,  1.68190537e-01, -2.69496934e-01, -3.04754810e-01,
       -2.62099948e-01,  1.36922064e-01,  3.93391710e-01, -8.12181958e-01,
       -3.15903057e-01,  1.10533721e-01, -2.50864695e-01,  6.56386891e-02,
       -2.37497275e-01, -6.04398679e-01, -5.32560555e-01, -1.62669444e-01,
       -4.45701188e-01, -1.67952827e-01, -5.07722157e-01, -8.51854037e-02,
       -1.35821066e+00, -3.39440908e-01, -6.41656480e-01, -9.51452941e-01,
       -8.23083293e-01, -5.69844420e-01, -1.04400576e-01, -3.71350420e-01,
       -8.65458568e-01, -2.64222047e-01,  8.06578890e-04, -5.68337779e-01,
       -6.25077227e-01, -1.00012510e+00,  7.59518951e-01,  2.46827712e-01,
        5.70561107e-01,  6.52003162e-01,  6.73384341e-01,  8.04894781e-01,
        6.34541410e-01,  3.63315273e-01,  4.36242632e-01,  3.31473086e-01,
        5.18170902e-01,  6.00943305e-01,  7.09919182e-01,  5.42156544e-01,
        5.96010036e-01,  6.32350524e-01,  5.11792431e-01,  7.26352234e-01,
        5.24889933e-01,  6.33500447e-01,  7.76251971e-01,  7.53647980e-01,
        3.36153030e-01,  8.15441364e-01,  5.57272462e-01,  1.44661947e-01,
        6.16792976e-01,  6.91086153e-01,  6.87286823e-01,  3.98429153e-01,
        1.07054311e+00,  6.24690432e-01,  6.84406406e-01,  7.26905590e-01,
        3.09651946e-01,  7.78085930e-01,  3.60425786e-01,  6.33481589e-01,
        3.17310855e-01,  6.56363955e-01,  6.10979533e-01,  8.08938177e-01,
        7.71336408e-01,  6.11452930e-01,  5.03192825e-01,  5.66868324e-01,
        7.22434360e-01,  5.64687162e-01,  5.11510302e-01,  7.02255988e-01]
)
# fmt: on
print(data_samples.shape)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(100,)
</pre></div>
</div>
</div>
</div>
<p>This is our dataset. It consists of 100 real numbers that are mostly in the interval [-1, 1]. After plotting a histogram of the data, we realized that it might be distributed according to a gaussian distribution. This is why we defined our model to consist of the sum of two parametrized gaussians:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def gaussian(x, mu, sigma):
    return np.exp(-0.5 * ((x - mu) / sigma) ** 2.0) / (sigma * np.sqrt(2 * np.pi))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def model_density_function(x_points, mus, sigmas):
    density_left = gaussian(x_points, mus[0], sigmas[0])
    density_right = gaussian(x_points, mus[1], sigmas[1])
    return (density_left + density_right) / 2
</pre></div>
</div>
</div>
</div>
<p>As the initial guesses to the model parameters, we chose <span class="math notranslate nohighlight">\(\mu_1=-0.3, \mu_2 = 0.6\)</span>, which are close to the peaks that we observe in the histogram with wider bins. For the width of the gaussians,we picked <span class="math notranslate nohighlight">\(\sigma_1=\sigma_2=0.2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_mus = [-0.3, 0.6]
model_sigmas = [0.2, 0.2]
</pre></div>
</div>
</div>
</div>
<p>When we input these values for <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> into the model density function, and then scan values of <span class="math notranslate nohighlight">\(x\)</span>, we get the following plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(1, 1)

x_points = np.linspace(-1.5, 1.5, 1000)

plt.hist(
    data_samples,
    bins=15,
    density=True,
    alpha=0.5,
    edgecolor=&quot;royalblue&quot;,
    linewidth=2,
    label=&quot;Training Data&quot;,
)
plt.plot(
    x_points,
    model_density_function(x_points, model_mus, model_sigmas),
    color=&quot;red&quot;,
    linewidth=2,
    label=&quot;Model&quot;,
)

plt.legend(fontsize=15)
ax.set_ylabel(&quot;Density&quot;, fontsize=16)
ax.set_xlabel(&quot;Data Value&quot;, fontsize=16)
ax.tick_params(labelsize=14)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/generative_modeling_102_11_0.png" src="../_images/generative_modeling_102_11_0.png" />
</div>
</div>
</div>
<div class="section" id="learning-by-training-the-generative-model">
<h2>Learning by training the generative model<a class="headerlink" href="#learning-by-training-the-generative-model" title="Permalink to this headline">¶</a></h2>
<p>Given this dataset is relatively simple compared to real-world cases, we may wonder how to find good parameters for machine learning models in general. This is done via <strong>learning</strong>. Learning (oftentimes also called “training”) a model is commonly done by tuning the parameters of the model to minimize a loss function (oftentimes also called “cost function”). The loss function is designed to describe how well your model fits the data you were given.</p>
<p>A loss function for generative models, for example, should measure the discrepancy between the samples from the dataset and the samples generated by our model. In other words, you should not be able to tell by looking at a sample whether it was part of the dataset or generated from the model.</p>
<p>One popular loss function is the KL divergence</p>
<div class="math notranslate nohighlight">
\[\text{KL}(p_{data}, p_{model}) = \sum_{x\sim \mathcal{D}} \left[ p_{data}(x) \log(p_{data}(x)) - p_{data}(x) \log(p_{model}(x)) \right]\]</div>
<p>In Python code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def kl_divergence(px_data, px_model):
    # px_data and px_model are numpy arrays
    return np.mean(px_data * np.log(px_data) - px_data * np.log(px_model))
</pre></div>
</div>
</div>
</div>
<p>By tuning the model parameters to decrease the KL divergence, we tend to obtain a model which generates samples that are more similar to the training data. You can imagine that every model with a collection of parameters has one or more constellations of parameters such that the loss value is minimal. Further improving it could either be impossible because it is the absolute minimal possible value (0 in the case of the KL divergence), or it may require a different, more potent model. Interestingly, we might not want a perfect fit on the training data. The keyword here is <em>generalization</em>, but more on that later. For now, we will try to lower the KL divergence of our model relative to the training dataset as much as we can.</p>
<p>First, for convenience, lets put all parameters into one array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_parameters = np.array(
    [
        model_mus[0],
        model_mus[1],
        model_sigmas[0],
        model_sigmas[1],
    ]
)
print(model_parameters)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.3  0.6  0.2  0.2]
</pre></div>
</div>
</div>
</div>
<p>Now, lets see how we could calculate the empirical data distribtion <span class="math notranslate nohighlight">\(p_{\mathcal{D}}(x)\)</span> given the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. But wait, why <span class="math notranslate nohighlight">\(p_{\mathcal{D}}(x)\)</span>? The above formula for the KL divergence requires <span class="math notranslate nohighlight">\(p_{data}(x)\)</span>, which is the true probability distribution underlying the training data. Unfortunately, the dataset is all we have and we have to use it as a substitute for the unknown <span class="math notranslate nohighlight">\(p_{data}(x)\)</span>.</p>
<p>So, how can we estimate <span class="math notranslate nohighlight">\(p_{\mathcal{D}}(x)\)</span>? One idea is to count the number of data samples which are inside a small interval <span class="math notranslate nohighlight">\([x-b, x+b]\)</span> for a small <span class="math notranslate nohighlight">\(b\)</span>, and divide that number by the total number of data samples. This is connected to the integral <span class="math notranslate nohighlight">\(p_{\mathcal{D}}(x) \sim \int \rho_{\mathcal{D}}(x) dx\)</span> of the empirical data density <span class="math notranslate nohighlight">\(\rho_{\mathcal{D}}(x)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>b = 1e-3
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def calculate_px_data(x):
    points_in_interval = (data_samples &gt;= x - b / 2) * (data_samples &lt; x + b / 2)
    return sum(points_in_interval) / len(data_samples)
</pre></div>
</div>
</div>
</div>
<p>As for our gaussian model, we apply a more sophisticated version of the numerical integration which is enabled by the ability to precisely calculate the density function <span class="math notranslate nohighlight">\(\rho_{model}(x)\)</span> for any <span class="math notranslate nohighlight">\(x\)</span>. The integration is then done using the <a class="reference external" href="https://en.wikipedia.org/wiki/Numerical_integration"><em>trapezoidal rule</em></a>:</p>
<div class="math notranslate nohighlight">
\[ p_{model}(x) \approx \int_{-b}^{b} \rho_{model}(x+s) ds \approx b\cdot\frac{\rho_{model}(x+b) + \rho_{model}(x-b)}{2}\]</div>
<p>This may appear a little cumbersome, but just keep in mind that here we are implementing the necessary numerics to estimate <span class="math notranslate nohighlight">\(p_{model}(x)\)</span> for this specific gaussian model. If we were instead utilizing a neural network or a parametrized quantum circuit, we could (or would need to) do it differently. In general, each choice of parametrized model allows or requires different methods for estimating the current probability distribution. Interestingly, some generative model do not allow the <em>explicit</em> calculation of  <span class="math notranslate nohighlight">\(p_{model}(x)\)</span>, but they can be trained using clever techniques. GANs are one such example! For further reading on <em>implicit</em> generative models, we refer to <a class="reference external" href="https://arxiv.org/abs/1610.03483">this paper</a>.</p>
<p>Lets code our case up:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def calculate_px_model(x, model_parameters):
    mus = model_parameters[:2]
    sigmas = model_parameters[2:4]

    density_left = model_density_function(x - b / 2, mus, sigmas)
    density_right = model_density_function(x + b, mus, sigmas)
    px_model = b * (density_right + density_left) / 2

    return px_model
</pre></div>
</div>
</div>
</div>
<p>To train the parameters of our model, we need an optimization algorithm. Here, we will use a simple gradient descent algorithm that is based on finite-difference gradients. The finite-difference gradient is defined via</p>
<div class="math notranslate nohighlight">
\[ grad(\text{KL})(x) \equiv \nabla \text{KL}(x) \approx \frac{\text{KL}(x+\epsilon) - \text{KL}(x+\epsilon)}{2\epsilon} \]</div>
<p>with a small <span class="math notranslate nohighlight">\(\epsilon\)</span> (epsilon).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def calculate_gradient(model_parameters, finite_distance_eps):
    gradient_vector = np.zeros(len(model_parameters))

    for kk in range(len(model_parameters)):
        delta_vector = np.zeros(len(model_parameters))
        delta_vector[kk] = finite_distance_eps

        params_plus = model_parameters + delta_vector
        params_minus = model_parameters - delta_vector

        px_data = np.array([calculate_px_data(x) for x in data_samples])

        px_model_plus = np.array(
            [calculate_px_model(x, params_plus) for x in data_samples]
        )
        px_model_minus = np.array(
            [calculate_px_model(x, params_minus) for x in data_samples]
        )

        kl_plus = kl_divergence(px_data, px_model_plus)
        kl_minus = kl_divergence(px_data, px_model_minus)

        gradient = (kl_plus - kl_minus) / (2 * finite_distance_eps)
        gradient_vector[kk] = gradient

    return gradient_vector
</pre></div>
</div>
</div>
</div>
<p>Choose <span class="math notranslate nohighlight">\(\epsilon = 10^{-3}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>eps = 1e-3
</pre></div>
</div>
</div>
</div>
<p>As for the gradient descent, we perform <span class="math notranslate nohighlight">\(T=100\)</span> iterations, and the following update to the model parameters <span class="math notranslate nohighlight">\(\theta^{(t)}\)</span> is performed at every iteration <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \theta^{(t+1)} = \theta^{(t+1)} - \eta\cdot \nabla\text{KL}(x; \theta^{(t)}),\]</div>
<p>with a <em>learning rate</em> <span class="math notranslate nohighlight">\(\eta\)</span> (eta).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%%time

learning_rate = 0.2
T = 100

all_kls = []

# evaluate and record the initial KL divergence loss
initial_model_parameters = model_parameters.copy()
px_data = np.array([calculate_px_data(x) for x in data_samples])
px_model = np.array(
    [calculate_px_model(x, initial_model_parameters) for x in data_samples]
)
all_kls.append(kl_divergence(px_data, px_model))

for it in range(T):
    # one gradient descent step
    gradient_vector = calculate_gradient(model_parameters, eps)
    model_parameters -= learning_rate * gradient_vector

    # evaluate and record the KL divergence loss
    px_data = np.array([calculate_px_data(x) for x in data_samples])
    px_model = np.array([calculate_px_model(x, model_parameters) for x in data_samples])
    all_kls.append(kl_divergence(px_data, px_model))
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Wall time: 10.1 s
</pre></div>
</div>
</div>
</div>
<p>Plot up the loss progression of over the training iterations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(1, 1)

x_points = np.linspace(-1.5, 1.5, 1000)

plt.plot(all_kls, linewidth=3)

ax.set_ylabel(&quot;KL Divergence&quot;, fontsize=16)
ax.set_xlabel(&quot;Iteration $t$&quot;, fontsize=16)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/generative_modeling_102_30_0.png" src="../_images/generative_modeling_102_30_0.png" />
</div>
</div>
<p>We see that the KL divergence loss value is slowly plateauing, which means that our model has converged. Now, compare the initial model distribution to the final distribution and the training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(1, 1)

points = np.linspace(-1.5, 1.5, 1000)

ax.hist(
    data_samples,
    bins=100,
    density=True,
    alpha=0.5,
    edgecolor=&quot;royalblue&quot;,
    linewidth=1,
    label=&quot;Training Data&quot;,
)
ax.plot(
    points,
    model_density_function(
        points, initial_model_parameters[:2], initial_model_parameters[2:]
    ),
    color=&quot;red&quot;,
    linewidth=3,
    label=&quot;Initial Model&quot;,
)
ax.plot(
    points,
    model_density_function(points, model_parameters[:2], model_parameters[2:]),
    color=&quot;orange&quot;,
    linewidth=3,
    label=&quot;Final Model&quot;,
)
ax.legend(fontsize=15)
ax.set_xlabel(&quot;Data Value&quot;, fontsize=16)
ax.set_ylabel(&quot;Density&quot;, fontsize=16)
ax.tick_params(labelsize=14)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/generative_modeling_102_32_0.png" src="../_images/generative_modeling_102_32_0.png" />
</div>
</div>
<p>While our inital parameter guess were quite good, the optimized parameters appear to define a model which describes the data much better. But let us not just guess. We (here at <a class="reference external" href="http://alqor.io">alqor.io</a>) know the <em>true</em> distribution from which the data was sampled. The “correct” parameters were:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>mus = [-0.4, 0.6]
sigmas = [0.3, 0.15]
</pre></div>
</div>
</div>
</div>
<p>When comparing this to our final trained parameters,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>model_mus = model_parameters[:2]
print(&quot;model mus:&quot;, model_mus)
model_sigmas = model_parameters[2:]
print(&quot;model sigmas:&quot;, model_sigmas)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>model mus: [-0.33731584  0.60914317]
model sigmas: [0.3387956  0.14697371]
</pre></div>
</div>
</div>
</div>
<p>we don’t see a perfect match - but it’s really close! And this is given only 100 samples, that lie rather sparsely in the domain <span class="math notranslate nohighlight">\(x \in [-1.5, 1.5]\)</span>.</p>
<p>Plotting all distributions together makes it much clearer how much the training improved our model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, ax = plt.subplots(1, 1)

points = np.linspace(-1.5, 1.5, 1000)

ax.hist(
    data_samples,
    bins=100,
    density=True,
    alpha=0.5,
    edgecolor=&quot;royalblue&quot;,
    linewidth=1,
    label=&quot;Training Data&quot;,
)
ax.plot(
    points,
    model_density_function(
        points, initial_model_parameters[:2], initial_model_parameters[2:]
    ),
    color=&quot;red&quot;,
    linewidth=3,
    label=&quot;Initial Model&quot;,
)
ax.plot(
    points,
    model_density_function(points, model_parameters[:2], model_parameters[2:]),
    color=&quot;orange&quot;,
    linewidth=3,
    label=&quot;Final Model&quot;,
)
ax.plot(
    points,
    model_density_function(points, mus, sigmas),
    color=&quot;Black&quot;,
    linewidth=3,
    label=&quot;True Distribution&quot;,
)
ax.legend(fontsize=15)
ax.set_xlabel(&quot;Data Value&quot;, fontsize=16)
ax.set_ylabel(&quot;Density&quot;, fontsize=16)
ax.tick_params(labelsize=14)
plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/generative_modeling_102_39_0.png" src="../_images/generative_modeling_102_39_0.png" />
</div>
</div>
<p>Lets sample 100 times from our newly trained model, and decide for yourself if you can spot a systematic difference:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def sample_model_distribution(n_samples, mus, sigmas):
    n_left_samples = n_samples // 2
    n_right_samples = n_samples - n_left_samples

    samples = np.append(
        np.random.normal(mus[0], sigmas[0], size=n_left_samples),
        np.random.normal(mus[1], sigmas[1], size=n_right_samples),
    )
    return samples
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>np.random.seed(42)
generated_samples = sample_model_distribution(100, model_mus, model_sigmas)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.hist(
    data_samples,
    bins=15,
    density=True,
    alpha=0.5,
    edgecolor=&quot;royalblue&quot;,
    linewidth=2,
    label=&quot;Training Data&quot;,
)
ax1.hist(
    generated_samples,
    bins=15,
    density=True,
    alpha=0.5,
    edgecolor=&quot;red&quot;,
    linewidth=2,
    label=&quot;Generated Data&quot;,
)
ax2.hist(
    data_samples,
    bins=100,
    density=True,
    alpha=0.5,
    edgecolor=&quot;royalblue&quot;,
    linewidth=1,
    label=&quot;Training Data&quot;,
)
ax2.hist(
    generated_samples,
    bins=100,
    density=True,
    alpha=0.5,
    edgecolor=&quot;red&quot;,
    linewidth=1,
    label=&quot;Generated Data&quot;,
)

ax1.set_title(&quot;Wider Histogram Bins&quot;, fontsize=16)
ax2.set_title(&quot;Thinner Histogram Bins&quot;, fontsize=16)
ax1.set_xlabel(&quot;Data Value&quot;, fontsize=16)
ax1.set_ylabel(&quot;Density&quot;, fontsize=16)
ax2.set_xlabel(&quot;Data Value&quot;, fontsize=16)
ax2.set_ylabel(&quot;Density&quot;, fontsize=16)
ax1.tick_params(labelsize=14)
ax2.tick_params(labelsize=14)

plt.show()
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/generative_modeling_102_43_0.png" src="../_images/generative_modeling_102_43_0.png" />
</div>
</div>
<p>This looks like a high-quality approximation of the true underlying distribution. But is the KL divergence loss is not 0. Is that a problem? No! In fact, this is one of the things that makes ML in general, but generative ML in particular, challenging. We don’t want to completely “memorize” the training data with our model - we want to <em>generalize</em> to parts of the data space that we did not observe. If we modeled the data with, for example, 20 overlapping gaussians, we would likely achieve lower KL divergence values, but the comparison with the true distribution would be worse. This is called <em>overfitting</em>. In our example, we circumvented overfitting by defining a model which is highly specific to the structure we observed in the data, and has only few tunable model parameters.</p>
</div>
<div class="section" id="summary-and-outlook">
<h2>Summary and outlook<a class="headerlink" href="#summary-and-outlook" title="Permalink to this headline">¶</a></h2>
<p>Let us recap:</p>
<p>In a generative machine learning task, you receive data, and the goal is to learn a model of the true underlying probability distribution of the data, i.e., you want to learn the distribution from which the data itself was sampled from. Then, you want to be able to generate new data from this model, which has similar characteristics, but is not inside the training set.<br />
The moving pieces in such a task are:</p>
<ul class="simple">
<li><p><strong>The dataset</strong>. How does it look like?</p></li>
<li><p><strong>The model</strong>. Is it flexible enough to approximate the data distribution well? Or is it highly specialized like in our gaussian case?</p></li>
<li><p><strong>The loss function</strong>. How do you estimate the distance the training data samples and your model distribution?</p></li>
<li><p><strong>The optimization algorithm</strong>. How do you get from (potentially very bad) initial parameters to a good set of parameters?</p></li>
</ul>
<p>In the following sessions, you will see how we replace <strong>the model</strong> with a parameterized quantum circuit. This brings about new choices and challenges, but also new interesting opportunities. See you soon!</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "Alqor-UG/quantum-tutorials",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./generative_modeling"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="generative_modeling_101.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Tutorial 1 – A baby example of generative modeling</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="generative_modeling_103.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tutorial 3 – An untrained generative model with qubits</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Quantum Tutorials Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>